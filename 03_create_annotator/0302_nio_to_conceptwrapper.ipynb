{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd1caab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This notebook is used to turn the NIO.owl into the .xml format required as the input of the ConceptMapper.\n",
    "\n",
    "## input format:\n",
    "    <AnnotationAssertion>\n",
    "        <AnnotationProperty abbreviatedIRI=\"metadata:prefixIRI\"/>\n",
    "        <AbbreviatedIRI>obo:NBO_0000304</AbbreviatedIRI>\n",
    "        <Literal datatypeIRI=\"http://www.w3.org/2001/XMLSchema#string\">NBO:0000304</Literal>\n",
    "    </AnnotationAssertion>\n",
    "    <AnnotationAssertion>\n",
    "        <AnnotationProperty abbreviatedIRI=\"rdfs:label\"/>\n",
    "        <AbbreviatedIRI>obo:NBO_0000304</AbbreviatedIRI>\n",
    "        <Literal datatypeIRI=\"http://www.w3.org/2001/XMLSchema#string\">memory loss behavior</Literal>\n",
    "    </AnnotationAssertion>\n",
    "    <AnnotationAssertion>\n",
    "        <AnnotationProperty abbreviatedIRI=\"obo:synonym\"/>\n",
    "        <AbbreviatedIRI>obo:NBO_0000304</AbbreviatedIRI>\n",
    "        <Literal datatypeIRI=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#PlainLiteral\">forgetting</Literal>\n",
    "    </AnnotationAssertion>\n",
    "\n",
    "## expected format:\n",
    "<?xml version=\"1.0\" ?>\n",
    "<synonym>\n",
    "    <token id = \"ncbi xxxxx\", canonical=\"colon, nos\">\n",
    "         <variant base=\"colon, nos\"/>\n",
    "         <variant base=\"large intestine\"/>\n",
    "         <variant base=\"large bowel\"/>\n",
    "    </token>\n",
    "</synonym>\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "711cffc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from collections import defaultdict, Counter\n",
    "from xml.dom import minidom\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a6322e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "nio_path = \"/Users/yidesdo21/Projects/inputs/ontologies/NIO1.1.owl\"\n",
    "\n",
    "with open(nio_path) as f:\n",
    "    contents = f.read()\n",
    "#     print(contents)\n",
    "\n",
    "soup = BeautifulSoup(contents, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cff2e60a",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_ids = set()\n",
    "id_canonicals = list()\n",
    "id_variants = list()\n",
    "syns = [\"obo:synonym\", \"oboInOwl:hasExactSynonym\", \"oboInOwl:hasNarrowSynonym\", \n",
    "        \"NDDUO:Synonym\", \"oboInOwl:hasBroadSynonym\", \"oboInOwl:hasRelatedSynonym\"]\n",
    "token_ids_meta, token_ids_label = list(), list()\n",
    "\n",
    "for anno_assert in soup.find_all('annotationassertion'):\n",
    "    property_tag = anno_assert.annotationproperty\n",
    "    \n",
    "    # property_attri extracts \"rdfs:label\" in <AnnotationProperty abbreviatedIRI=\"rdfs:label\"/>\n",
    "    # it can also be <AnnotationProperty abbreviatedIRI=\"obo:synonym\"/>, etc. \n",
    "    property_attri = property_tag[\"abbreviatediri\"]    # str\n",
    "    \n",
    "    # iri_tag extracts \"obo:OBI_0002193\" in <AbbreviatedIRI>obo:OBI_0002193</AbbreviatedIRI>\n",
    "    iri_tag = anno_assert.abbreviatediri   # the iri tag. type(iri_tag) == Tag\n",
    "    \n",
    "    # only look at the <AbbreviatedIRI> structure, exclusing the <IRI>\n",
    "    if iri_tag is not None:\n",
    "        token_id = iri_tag.contents[0]\n",
    "        \n",
    "        if property_attri == \"metadata:prefixIRI\":  # point to the token id ## the number is smaller than the token_ids in the labels\n",
    "            token_ids_meta.append(token_id)\n",
    "   \n",
    "        if property_attri == \"rdfs:label\":   # point to the canonical\n",
    "            token_ids.add(token_id)   # # point to the token id\n",
    "            canonical = anno_assert.literal.get_text()\n",
    "#             canonical = anno_assert.literal.get_text().lower() \n",
    "            id_canonicals.append((token_id, canonical))\n",
    "#             id_canonicals.append((token_id, wnl.lemmatize(canonical)))  # has duplications, why?\n",
    "            token_ids_label.append(token_id)\n",
    "            # using set(id_canonicals) can filter the duplications\n",
    "#             print(anno_assert)\n",
    "#             print(\"----------\")\n",
    "          \n",
    "        if property_attri in syns:  # point to the variant base\n",
    "            variant = anno_assert.literal.get_text()\n",
    "#             variant = anno_assert.literal.get_text().lower() \n",
    "            id_variants.append((token_id, variant))\n",
    "#             id_variants.append((token_id, wnl.lemmatize(variant)))\n",
    "    \n",
    "token_ids = sorted(list(token_ids))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0858b923",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_id_canonicals = defaultdict(list)\n",
    "for token_id, cano in id_canonicals:\n",
    "    dict_id_canonicals[token_id].append(cano)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d86e9b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_id_variants = defaultdict(list)\n",
    "for token_id, variant in id_variants:\n",
    "    dict_id_variants[token_id].append(variant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "24a643e4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## only for <IRI> \n",
    "ids_iri = list()   # this list shouldn't be used in later processing because it is incomplete, use id_canonical instead\n",
    "id_cano_iri = list()\n",
    "id_var_iri = list()\n",
    "onto_matches = [\"AlzheimerOntology\", \"OntoDT\", \"obo\", \"bfo\", \"NDDO\"]\n",
    "\n",
    "for anno_assert in soup.find_all('annotationassertion'):\n",
    "    property_tag = anno_assert.annotationproperty\n",
    "    \n",
    "    # property_attri extracts \"rdfs:label\" in <AnnotationProperty abbreviatedIRI=\"rdfs:label\"/>\n",
    "    # it can also be <AnnotationProperty abbreviatedIRI=\"NDDUO:Synonym\"/>, etc. \n",
    "    property_attri = property_tag[\"abbreviatediri\"]    # str\n",
    "    \n",
    "    # tag_iri has a url, the last part is the token id (need preprocessing)\n",
    "    # e.g. <IRI>http://scai.fraunhofer.de/AlzheimerOntology#brainstem</IRI>\n",
    "    # replace the \"#\" sign as \":\" in <AlzheimerOntology#brainstem>, then it becomes the token id\n",
    "    iri_url = anno_assert.iri \n",
    "    \n",
    "    # only look at the <IRI> structure, excluding the <AbbreviatedIRI>\n",
    "    if iri_url is not None:\n",
    "        if property_attri in [\"metadata:prefixIRI\", \"NDDUO:Synonym\", \"rdfs:label\"]:   # have checked for the \"NDDUO:Synonym\", this is the only attribute that has the synonyms\n",
    "#         if property_attri == \"metadata:prefixIRI\":   # check if the guess of using <IRI> as the token id is true for those who are not from ADO. the answer is no \n",
    "            url = iri_url.contents[0]\n",
    "            split_url = url.split(\"/\")\n",
    "        \n",
    "            if len(split_url) > 3 and any(x in split_url[3] for x in onto_matches):   # dumped the unclear tokens\n",
    "                rev_fir, rev_sec, third = split_url[-1], split_url[-2], split_url[3]\n",
    "                len_split = len(split_url)\n",
    "                if len_split == 4:  \n",
    "                    token_id = rev_fir.replace(\"#\", \":\")  # For ADO and OntoDT\n",
    "                    \n",
    "                elif len_split == 5:   # only one AlzheimerOntology#NINCDS/ADRDA_criteria, three duplications  \n",
    "                    if rev_sec == \"AlzheimerOntology#NINCDS\": # for ADO\n",
    "                        token_id = (rev_sec+\"_\"+rev_fir).replace(\"#\",\":\")   \n",
    "                    elif rev_sec == \"OntoDT\" or rev_sec == \"NDDO\":  # for OntoDT and NDDO\n",
    "                        token_id = rev_sec+\":\"+rev_fir\n",
    "                    elif rev_sec == \"obo\":  # for obo-fma, obo-npt\n",
    "                        token_id = rev_sec+\":\"+rev_fir.replace(\"#\", \"_\").replace(\".owl\", \"\")\n",
    "\n",
    "                elif len_split == 6: # for obo-bahavior, bfo, NDDO\n",
    "                    if rev_sec == \"DMtypes\":   # exclude DMtypes, not in the OntoDT.owl\n",
    "                        continue    \n",
    "                    if third == \"ofo\":\n",
    "                        token_id = third+\":\"+rev_fir\n",
    "                    elif third == \"bfo\":\n",
    "                        token_id = third+\":\"+rev_fir.replace(\"#\", \"_\")\n",
    "                    elif third == \"NDDO\":\n",
    "                        token_id = third+\":\"+rev_sec+\"_\"+rev_fir\n",
    "                        \n",
    "                elif len_split == 8:  # for OntoDT#OntoDT_184436\n",
    "                    token_id = rev_fir.replace(\"#\", \":\")\n",
    "            \n",
    "                    \n",
    "                if property_attri == \"metadata:prefixIRI\":    # using this attribute to extract token ids is incomplete\n",
    "                    ids_iri.append(token_id)\n",
    " \n",
    "                if property_attri == \"rdfs:label\":   # point to the canonical\n",
    "                    canonical = anno_assert.literal.get_text()\n",
    "#                     canonical = anno_assert.literal.get_text().lower()\n",
    "                    if token_id.split(\":\")[0] == \"OntoDT\":\n",
    "                        cano_onto = canonical.split(\":\")\n",
    "                        if len(cano_onto) > 1:\n",
    "                            canonical = cano_onto[1].lstrip()\n",
    "                    id_cano_iri.append((token_id, canonical))\n",
    "#                     id_cano_iri.append((token_id, wnl.lemmatize(canonical)))  \n",
    "           \n",
    "                if property_attri == \"NDDUO:Synonym\":  # point to the variant base\n",
    "                    variant = anno_assert.literal.get_text()\n",
    "#                     variant = anno_assert.literal.get_text().lower()\n",
    "                    if variant == \"http://www.ebi.ac.uk/efo/efo_0000493\":\n",
    "                        continue\n",
    "                    id_var_iri.append((token_id, variant))\n",
    "#                     id_var_iri.append((token_id, wnl.lemmatize(variant)))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "401d0242",
   "metadata": {},
   "outputs": [],
   "source": [
    "## turn the ontologies with <IRI> tag into id_canonical and id_variant dictionaries\n",
    "## add the ontologies to the existed dict_id_canonicals\n",
    "token_id_iri = list()\n",
    "\n",
    "for token_id, cano in id_cano_iri:\n",
    "    token_id_iri.append(token_id)\n",
    "    dict_id_canonicals[token_id].append(cano)\n",
    "    \n",
    "for token_id, variant in id_var_iri:\n",
    "    dict_id_variants[token_id].append(variant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3e7d2017",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create file\n",
    "xmldoc = minidom.Document()\n",
    "\n",
    "# creat root element\n",
    "root_element = xmldoc.createElement('synonym')\n",
    "xmldoc.appendChild(root_element)\n",
    "\n",
    "# combine token ids from both <AbbreviatedIRI> and <IRI> structures\n",
    "token_ids.extend(list(set(token_id_iri)))   # don't use ids_iri, not every canonical has m\"eta:prefixiri\" attribute, use in the token ids in the id_canonical dictionary\n",
    "\n",
    "for token_id in token_ids:\n",
    "    productChild = xmldoc.createElement('token')\n",
    "    productChild.setAttribute('id', token_id)  # attribute, value\n",
    "    \n",
    "    canonical = dict_id_canonicals.get(token_id, None)\n",
    "    if canonical is not None:\n",
    "        productChild.setAttribute('canonical', canonical[0])  # take the first canonical if one token_id has multiple canonicals\n",
    "        variant_list = canonical[1:]  # add the rest of the canonicals as the variant\n",
    "    else:\n",
    "#         print(token_id)   # the print statement prints nothing, it won't get into the else statement \n",
    "        continue\n",
    "        \n",
    "    root_element.appendChild(productChild) \n",
    "\n",
    "    variant = dict_id_variants[token_id]\n",
    "    variant_list.extend(variant)\n",
    "    \n",
    "    if variant_list is not None:\n",
    "        variant_list = list(set(variant_list)) # add the canonicals to the variant, eliminate the duplications\n",
    "    else:\n",
    "        variant_list = variant  # when the token_id only have one canonical and have no variants\n",
    "    \n",
    "    variant_list.append(canonical[0])  # the canonical has to be in the variant; I don't want to mess up with the above codes so I add the canonical in the end of the variant list\n",
    "    \n",
    "    variant_list = list(set(variant_list))  # avoid duplications\n",
    "    \n",
    "    for user in range(len(variant_list)):\n",
    "\n",
    "        # create child element\n",
    "        product_grandChild = xmldoc.createElement('variant')\n",
    "\n",
    "        # insert user data into element\n",
    "        product_grandChild.setAttribute('base', variant_list[user])\n",
    "        productChild.appendChild(product_grandChild)    \n",
    "\n",
    "    \n",
    "xml_str = xmldoc.toprettyxml(indent =\"\\t\")\n",
    "\n",
    "# save file\n",
    "dict_path = \"/Users/yidesdo21/Projects/inputs/dictionary/\"\n",
    "save_path_file = \"nio_case.xml\"   # have to be very careful for this, this is not the final xml dictionary \n",
    "\n",
    "with open(dict_path+save_path_file, \"w\") as f:\n",
    "    f.write(xml_str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d6f688fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3755"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(token_ids)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
