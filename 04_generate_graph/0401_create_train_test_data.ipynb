{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4dd443a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This notebook aims to create the train/test pairs for the experiment.\n",
    "==============================================\n",
    "- Each train/test pair will have a feature network and a label network \n",
    "for the training data, and a feature network and a label network for the testing data.\n",
    "- Nodes will be the nodes in the feature network in the training data.\n",
    "- Pre-processing: Nodes with degree more than 2000 are filtered. \n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "54523992",
   "metadata": {},
   "outputs": [],
   "source": [
    "from create_edge import create_yr_link\n",
    "import json\n",
    "from collections import defaultdict,Counter\n",
    "import numpy as np\n",
    "from imblearn.under_sampling import RandomUnderSampler "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a3f891d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the metadata from the json\n",
    "meta_path = \"/Users/yidesdo21/Projects/outputs/12_time_slicing/metadata/\"\n",
    "\n",
    "with open(meta_path+\"articles_with_anno.json\") as f:\n",
    "    metadata = json.load(f)\n",
    "\n",
    "year_edge,year_ln = create_yr_link(metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3039f79a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## prepare the raw corpus\n",
    "# 131 nodes have degrees more than 2000, these nodes are filtered in the graph\n",
    "hubs_2000 = {'AlzheimerOntology:cerebrospinal_fluid', 'NDDUO:Enzymes', '10090', '54226', 'NDDUO:Cluster', 'obo:BFO_0000066', 'AlzheimerOntology:presenilin_1', 'MESH:D019636', 'NDDUO:Mouse_', 'AlzheimerOntology:Subtypes', 'AlzheimerOntology:protein', 'AlzheimerOntology:t_tau', 'NDDUO:Risk_factor', 'obo:RO_0000079', 'AlzheimerOntology:t_tau_Amyloid_beta_42', 'obo:BFO_0000015', 'NDDUO:Exposure', 'obo:ND_0003000', 'snap:Function', 'AlzheimerOntology:Brain_Imaging', 'NDDUO:male', 'obo:FMA_62493', 'AlzheimerOntology:vascular_dementia', 'MESH:D003072', 'NDDUO:Intervention', 'AlzheimerOntology:Memory_orientation_screening_test', 'obo:OGMS_0000023', 'AlzheimerOntology:Oxidative_stress', 'snap:Role', 'AlzheimerOntology:Microglia', 'MESH:D064420', 'AlzheimerOntology:Membrane', 'NDDUO:disorder', 'obo:UO_0000033', 'NDDUO:biomarker', 'AlzheimerOntology:brain', 'AlzheimerOntology:control_group_study_arm', 'NDDUO:Distribution', 'NDDUO:Clinical', 'AlzheimerOntology:presenilin', 'NDDUO:In_vivo_models', 'AlzheimerOntology:secretase', 'MESH:D000544', 'AlzheimerOntology:APP', 'MESH:D014867', 'AlzheimerOntology:cortex', 'AlzheimerOntology:disease_onset', 'obo:UO_0000036', 'obo:UO_0000035', 'AlzheimerOntology:Mild_cognitive_Impairment', 'obo:IAO_0000651', 'span:Process', 'obo:BFO_0000023', 'NDDUO:Cell_death', 'obo:ND_0000113', 'obo:OGMS_0000031', 'NDDUO:Age', 'obo:BFO_0000034', 'AlzheimerOntology:antibody', 'obo:IAO_0000230', '11820', 'NDDUO:disease', 'AlzheimerOntology:senile_dementia', 'NDDUO:gene', 'NDDUO:drug', 'AlzheimerOntology:Moderate_cognitive_Decline', 'AlzheimerOntology:ApoE_Protein', 'AlzheimerOntology:lipid', 'NDDUO:Rat', 'AlzheimerOntology:presence_of_amyloid_plaque', 'AlzheimerOntology:age_risk_factor', 'NDDUO:model', 'AlzheimerOntology:amyloid_precursor_protein', 'AlzheimerOntology:Phosphorylation', 'snap:Site', 'AlzheimerOntology:Cognitive_Tests', 'AlzheimerOntology:ratio', 'obo:FMA_50801', 'AlzheimerOntology:advanced_glycation_end_product', '9606', 'AlzheimerOntology:Receptors', 'obo:PATO_0000011', 'AlzheimerOntology:inflammation', 'AlzheimerOntology:frontotemporal_dementia', 'obo:GO_0007612', 'MESH:D009410', 'NDDUO:Mild', 'NDDUO:Sampling', 'AlzheimerOntology:senile_plaque', 'MESH:D003704', 'NDDUO:In_vivo', 'AlzheimerOntology:Hippocampus', '351', 'NDDUO:neurodegenerative_disease', 'obo:ND_0000152', 'obo:GO_0007613', 'NDDUO:Proportion', 'AlzheimerOntology:Late_Onset_Alzheimer_s_Disease', 'obo:NBO_0000215', 'NDDUO:In_vitro', 'NDDUO:Study_type', 'MESH:D008569', 'obo:OBI_0000753', 'obo:NBO_0000282', 'NDDUO:Neurons', 'AlzheimerOntology:amyloid_beta_protein', 'AlzheimerOntology:Fibrils', 'MESH:D020258', '10116', 'AlzheimerOntology:amyloid_beta_deposition', 'obo:OBI_0000070', 'AlzheimerOntology:Gama_secretase', 'NDDUO:rate', 'AlzheimerOntology:enzyme', 'AlzheimerOntology:Apoptosis', 'obo:IDO_0000666', 'AlzheimerOntology:mutation', 'obo:FMA_61109', '4137', 'obo:IAO_0000101', 'obo:RO_0000081', 'AlzheimerOntology:Abeta_42', 'MESH:D003643', 'obo:NPT_0015009', 'NDDUO:inhibitor', 'obo:RO_0002211', 'NDDUO:Stages', 'AlzheimerOntology:mental_disorder', 'NDDUO:Pathogenesis', 'AlzheimerOntology:Intervention', 'AlzheimerOntology:neurotoxic'}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bdc7b0e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_indices(year_edge):\n",
    "    \"\"\"create a uniform indices for all nodes in all networks, use this globally.\n",
    "    indices starting with 0, to fit with CSVDataset in DGL library,\n",
    "    input -- year_edge: a dictionary with {year:(set_of_links)},\n",
    "    output -- a dictionary with {entity_name1:entity_index1: ...}\"\"\"\n",
    "    \n",
    "    vertices = {} # lookup table for entity name and entity index\n",
    "    node_index = -1  # index range [0,len_node-1]\n",
    "    \n",
    "    for yr, es in year_edge.items():   \n",
    "        for e in es:\n",
    "            entity1,entity2 = e[0],e[1]\n",
    "\n",
    "            if entity1 not in vertices:\n",
    "                node_index += 1\n",
    "                vertices[entity1] = node_index  \n",
    "\n",
    "            if entity2 not in vertices:\n",
    "                node_index += 1\n",
    "                vertices[entity2] = node_index\n",
    "\n",
    "            \n",
    "                \n",
    "    print(\"\\n%s vertices in the whole network read.\" % (len(vertices)))      \n",
    "    return vertices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "37ba215f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_nodes(year_edge,g_0, g_t, hubs=hubs_2000):\n",
    "    \"\"\"node control when constructing the training data and the testing data,\n",
    "        the links for both feature network and label network have to come from nodes in the feature network only\n",
    "        otherwise the features can't be extracted,\n",
    "        under this setting, g_t will be the end of feature network from the training data\n",
    "        --- update: we add one experiment with new nodes considered, \n",
    "                    under this setting, g_t will be the end of label network from the testing data\n",
    "        use this function as a fitler\n",
    "      input -- g_0: the start year, g_t: end of the feature network,\n",
    "               year_edge: a dictionary with {year:(set_of_links)}\n",
    "      output -- a set of entities only appear in the feature network\n",
    "    \"\"\"\n",
    "    entities = set()\n",
    "    \n",
    "    for yr, es in year_edge.items():\n",
    "        if g_0 <= yr <= g_t: \n",
    "            for e in es:\n",
    "                entity1,entity2 = e[0],e[1]   \n",
    "                \n",
    "#                 if entity1 == \"MESH:D018698\" or entity2 == \"MESH:D018698\":\n",
    "#                     print(e)\n",
    "                \n",
    "                if entity1 in hubs or entity2 in hubs:\n",
    "                    continue\n",
    "                    \n",
    "                else:\n",
    "                    entities.add(entity1)\n",
    "                    entities.add(entity2)\n",
    "        \n",
    "    return entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f49b4596",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "g_0 = 1997\n",
    "g_t = 2001\n",
    "\n",
    "for yr, es in year_edge.items():\n",
    "    if g_0 <= yr <= g_t: \n",
    "        for e in es:\n",
    "            entity1,entity2 = e[0],e[1]   \n",
    "\n",
    "            if entity1 == \"MESH:D003329\" or entity2 == \"MESH:D003329\":\n",
    "                print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "26ec1419",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ignore the indices for easy investigation for the high AUROC\n",
    "## note that the adjacency matrix doubles the links as it includes the duplications (a,b) and (b,a)\n",
    "def create_network(year_edge, g_0, g_t, fixed_nodes, ver_idx=\"\"):\n",
    "    \"\"\"create the feature network or the label network, \n",
    "        the links can only from the nodes in the feature network,\n",
    "       input -- g_0: start of the network, g_t: end of the network,\n",
    "             -- year_edge: links in the network\n",
    "             -- fixed_nodes: a set of entities only appear in the feature network\n",
    "             -- ver_idx: a dictionary with {entity_name1:entity_index1, ...},\n",
    "       output -- a dictionary {entity_index1:(entity_index3, ...),...}\n",
    "    \"\"\"\n",
    "    \n",
    "    adj_mat = defaultdict(set)\n",
    "    \n",
    "    for yr, es in year_edge.items():\n",
    "        if g_0 <= yr <= g_t: \n",
    "            for e in es:\n",
    "                entity1,entity2 = e[0],e[1]\n",
    "#                 print(entity1,entity2)\n",
    "\n",
    "\n",
    "                if entity1 in fixed_nodes and entity2 in fixed_nodes:\n",
    "                    adj_mat[entity1].add(entity2)\n",
    "                    adj_mat[entity2].add(entity1)\n",
    "#                 ent1_vertex_ind,ent2_vertex_ind = ver_idx.get(entity1),ver_idx.get(entity2)\n",
    "#                 adj_mat[ent1_vertex_ind].add(str(ent2_vertex_ind))  # node pairs can be duplicated in year_edge.items()\n",
    "#                 adj_mat[ent2_vertex_ind].add(str(ent1_vertex_ind))    \n",
    "    \n",
    "    return adj_mat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "06ee0c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "## the negative links have to obey the fixed nodes rule\n",
    "def create_negatives(year_edge, fea_pos_adj_mat, lab_pos_adj_mat, fixed_nodes, ver_idx=\"\"):\n",
    "    \"\"\"return all negatives in the label network, only the links with the nodes in the feature network are included\n",
    "        all negative cases in the feature network minus all new positive cases in the label network,\n",
    "       only the nodes from the feature network can be included,\n",
    "           fea_pos_adj_mat -- all positive cases in the feature network,\n",
    "           lab_pos_adj_mat -- all new positive cases in the label network,\n",
    "           fixed_nodes -- a set of entities only appear in the feature network,\n",
    "           ver_idx -- a dictionary {entity_name:entity_index},\n",
    "       output -- neg_label: a dictionary {entity1:(\"entity2\",...),...}\n",
    "                 all negative cases in the label network (excluding new nodes)\n",
    "       \"\"\"\n",
    "    \n",
    "    ## all negative cases in the feature network = all possible cases in the feature network - all positive cases in the feature network\n",
    "    # all possible cases in the feature network \n",
    "#     all_possible = set([str(ver_idx.get(n)) for n in fixed_nodes])\n",
    "    neg_feature = {}\n",
    "    neg_label = defaultdict(set)  # all negative cases in the label network (excluding new nodes) \n",
    "    node_pairs = set()  # for removing duplications (a,b) and (b,a)\n",
    "    \n",
    "    # all negative cases in the feature network\n",
    "    for idx1,neighs in fea_pos_adj_mat.items():\n",
    "        neighs_set = set(neighs)\n",
    "        neighs_set.add(idx1)\n",
    "        neg_feature[idx1] = fixed_nodes.difference(neighs_set)   # exclude itself     \n",
    "    \n",
    "    \n",
    "    ## all negative cases in the label network = all negative cases in the feature network - all new positive cases in the label network\n",
    "    # all keys/nodes are already filtered to be in the feature network in the create_label_network function\n",
    "    lab_pos_idxs = lab_pos_adj_mat.keys()\n",
    "\n",
    "    for idx2,negs in neg_feature.items():\n",
    "        if idx2 not in lab_pos_idxs:\n",
    "            for neg in negs:    ## check duplications (a,b) (b,a)\n",
    "                if (idx2,neg) in node_pairs:\n",
    "                    continue\n",
    "                if (neg,idx2) in node_pairs:\n",
    "                    continue\n",
    "\n",
    "                neg_label[idx2].add(neg)\n",
    "                node_pairs.add((idx2,neg))   \n",
    "\n",
    "        else:\n",
    "            idx2_pos_label = lab_pos_adj_mat.get(idx2)\n",
    "            negs_set = set(negs).difference(set(idx2_pos_label))\n",
    "            for neg2 in negs_set:    ## check duplications (a,b) (b,a)\n",
    "                if (idx2,neg2) in node_pairs:\n",
    "                    continue\n",
    "                if (neg2,idx2) in node_pairs:\n",
    "                    continue\n",
    "\n",
    "                neg_label[idx2].add(neg2)\n",
    "                node_pairs.add((idx2,neg2))             \n",
    "    \n",
    "    return neg_label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e55082d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_dup(lab_pos_adj_mat):\n",
    "    \"\"\"remove (b,a) if (a,b) exists for the positive links in feature networks,\n",
    "    input -- lab_pos_adj_mat, a dictionary with {entity_index1:(\"entity_index2\",...),...},\n",
    "    output -- a filterd label positive adjacency matrix, also a dictionary in the same format\"\"\"\n",
    "    node_pairs = set()\n",
    "    lab_pos_fil = defaultdict(set)   \n",
    "\n",
    "    for k,vs in lab_pos_adj_mat.items():\n",
    "        key = str(k)\n",
    "        for v in vs:\n",
    "            if (key,v) in node_pairs:\n",
    "    #             print(key,v)\n",
    "    #             print(\"---------\")\n",
    "                continue\n",
    "            if (v,key) in node_pairs:\n",
    "    #             print(key,v)\n",
    "    #             print(\"---------\")\n",
    "                continue\n",
    "\n",
    "            lab_pos_fil[key].add(v)\n",
    "            node_pairs.add((key,v))\n",
    "        \n",
    "    return lab_pos_fil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "40b2e01f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn all the pairs to a dictionary \n",
    "# Dict not really needed here but indexing in a dict is much faster than a list.\n",
    "# from: {entity_index1:(set_of_entity_indices),...}\n",
    "# to. : ('58920::snap:Function': 1, '58920::MESH:D012694': 1, ...) \n",
    "#.    1 for positive links, 0 for negative links\n",
    "\n",
    "def adj_mat_to_dict(adj_mat,label):\n",
    "    \"\"\"turn the adjacency matrix to a dictionary\n",
    "        input -- adj_mat: a dictionary,\n",
    "              -- label: 0 for negative labels, 1 for postiive labels,\n",
    "        output -- a dictionary {\"index1::index2\":positive_or_negative_link}\"\"\"\n",
    "    pair_dict = dict()\n",
    "\n",
    "    for k,v in adj_mat.items():\n",
    "#         entity1 = vertices_inv.get(k)\n",
    "#         entities = [vertices_inv[int(node_ind)] for node_ind in v]\n",
    "\n",
    "        for idx in v:\n",
    "            pair_dict[\"%s::%s\" % (k, idx)] = label\n",
    "    \n",
    "    return pair_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eed536b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# careful for the random state in sampling\n",
    "def sample_neg(lab_pos_adj_mat,lab_neg_adj_mat,sampling=True):\n",
    "    \"\"\"sample from lab_neg_adj_mat, making the negative link number equal to \n",
    "        the links in lab_pos_adj_mat,\n",
    "        input -- lab_pos_adj_mat: positive node pairs in the label network,  \n",
    "                 lab_neg_adj_mat: negative node pairs in the label network,\n",
    "        output -- X_res: sampled node pairs names, with 1:1 positive and negative labels\n",
    "                  y_res: labels for the node pairs\"\"\"\n",
    "\n",
    "#     lab_pos_dict = adj_mat_to_dict(adj_mat=remove_dup(lab_pos_adj_mat,label=1) )\n",
    "    lab_pos_dict = adj_mat_to_dict(adj_mat=remove_dup(lab_pos_adj_mat),label=1)  # already removed duplications\n",
    "    lab_neg_dict = adj_mat_to_dict(adj_mat=lab_neg_adj_mat,label=0)  # duplications should have been removed\n",
    "    lab_dict = {**lab_pos_dict,**lab_neg_dict}\n",
    "\n",
    "    X = np.array(list(lab_dict.keys())).reshape(-1, 1)\n",
    "    y = list(lab_dict.values())\n",
    "    print('Original dataset shape %s' % Counter(y))\n",
    "\n",
    "    if sampling is True:\n",
    "        undersam_neg = RandomUnderSampler()\n",
    "        X_res, y_res = undersam_neg.fit_resample(X, y)\n",
    "        print('Resampled dataset shape %s' % Counter(y_res))\n",
    "\n",
    "        return X_res, y_res\n",
    "\n",
    "    else:\n",
    "        return X,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "523e530e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a pipeline to create a training or testing data\n",
    "def data_pipeline(year_edge, year_ln, \n",
    "                   g_f0, g_ft, g_l0, g_lt, fixed_nodes,       \n",
    "                   hubs = hubs_2000,sampling=True,\n",
    "                   training=True):\n",
    "    \"\"\"create feature networks and label networks for both the training and testing network\n",
    "        input -- year_edge: edges in each year, a dictionary with {year:(set_of_links)},, \n",
    "                 year_ln: new edges in each year, \n",
    "                 fixed_nodes: fixed nodes for all networks in the training and testing data,\n",
    "                 g_f0: start of the feature network for both the training and testing networks,\n",
    "                 g_ft: end of the feature network, \n",
    "                 g_l0: start of the label network,\n",
    "                 g_lt: end of the label network,\n",
    "                 training: if training is True, then the edges to induce node embeddings will be returned,\"\"\"\n",
    " \n",
    "    ## step 1: create fixed nodes for the network\n",
    "    print(\"start of the feature network:\", g_f0)\n",
    "    print(\"end of the feature network:\", g_ft)\n",
    "    print(\"start of the label network:\", g_l0)\n",
    "    print(\"end of the label network:\", g_lt)    \n",
    "    print(\"-----------\")\n",
    "    \n",
    "#     fixed_test = fix_nodes(year_edge,g_f0,g_ft,hubs)\n",
    "    print(\"number of fixed nodes:\", len(fixed_nodes))\n",
    "    print(\"-----------\")\n",
    "    \n",
    "    ## step 2: create feature network\n",
    "    test_fea_net = create_network(year_edge,g_f0,g_ft,\n",
    "                                  fixed_nodes=fixed_nodes)\n",
    "    test_fea_cnt = dict()\n",
    "\n",
    "    for k,v in test_fea_net.items():\n",
    "        test_fea_cnt[k] = len(v)\n",
    "\n",
    "    # print(sum(test_fea_cnt.values()))   # duplications\n",
    "    print(\"positive links in the feature network:\", sum(test_fea_cnt.values())/2)   # without duplications\n",
    "    print(\"-----------\")\n",
    "    \n",
    "    ## step 3: create edges to achieve the similarity measures\n",
    "    test_fea_dict = adj_mat_to_dict(adj_mat=remove_dup(test_fea_net),label=1)\n",
    "    print(\"number of links to achieve the similarity measures:\", len(test_fea_dict))\n",
    "    print(\"-----------\")\n",
    "    \n",
    "    ## step 4: create label network\n",
    "    # positive cases in the label network are the new links in the label network\n",
    "    # need to exclude the links with new nodes\n",
    "    test_lab_net = create_network(year_ln,g_l0,g_lt, \n",
    "                                  fixed_nodes=fixed_nodes, ver_idx=\"\")\n",
    "    test_lab_cnt = dict()\n",
    "    \n",
    "#     print(fixed_test)\n",
    "#     print(test_lab_net)\n",
    "    for k,v in test_lab_net.items():\n",
    "        test_lab_cnt[k] = len(v)\n",
    "\n",
    "    # print(sum(test_lab_cnt.values()))   # this is doubled\n",
    "    print(\"positive links in the label network:\", sum(test_lab_cnt.values())/2)\n",
    "    print(\"-----------\")\n",
    "    \n",
    "    ## step 5: create feature+label network to induce the node embeddings \n",
    "    if training == True:\n",
    "        test_fea_lab_net = create_network(year_edge,g_f0,g_lt,\n",
    "                                          fixed_nodes=fixed_nodes)  \n",
    "        \n",
    "        debug1 = remove_dup(test_fea_lab_net)\n",
    "        debug2 = test_fea_lab_net\n",
    "        \n",
    "        test_fea_lab_dict = adj_mat_to_dict(adj_mat=remove_dup(test_fea_lab_net),label=1)\n",
    "        print(\"number of links to induce the node embeddings:\", len(test_fea_lab_dict))    \n",
    "        print(\"-----------\")\n",
    "    \n",
    "    ## step 6: create negative links\n",
    "    neg_label = create_negatives(year_edge, fea_pos_adj_mat=test_fea_net, \n",
    "                                 lab_pos_adj_mat=test_lab_net, fixed_nodes=fixed_nodes)\n",
    "    neg_label_cnt = dict()\n",
    "    \n",
    "    for k,v in neg_label.items():\n",
    "        neg_label_cnt[k] = len(v)\n",
    "\n",
    "    print(\"negative links in the feature network before undersampling:\", sum(neg_label_cnt.values()))   \n",
    "    # duplications removed, links from label network removed\n",
    "    print(\"-----------\")\n",
    "    \n",
    "    ## step 7: sample the negative links to maintain 1:1 for the positive links\n",
    "    X_res_test, y_res_test = sample_neg(test_lab_net,neg_label,sampling)\n",
    "    \n",
    "#     ## step 6: calculate model scores\n",
    "# #     print(test_fea_net)\n",
    "#     cn_score,jc_score,aa_score,pa_score = calculate_feature(X_res=X_res_test, \n",
    "#                                                                     adj_matrix=test_fea_net, \n",
    "#                                                                     y_res=y_res_test, vertices=\"\")\n",
    "    \n",
    "#     ## step 7: evaluate the models\n",
    "#     model_prc,model_roc = evaluate_models(cn_score,jc_score,aa_score,pa_score,y_res_test)\n",
    "#     print(\"----------------------\")\n",
    "    \n",
    "    \n",
    "#     records.append((len(fixed_test), sum(test_fea_cnt.values())/2,\n",
    "#                      sum(test_lab_cnt.values())/2, sum(neg_label_cnt.values()),\n",
    "#                      (model_prc,model_roc)))\n",
    "    if training == True:\n",
    "        return X_res_test, y_res_test, test_fea_dict, test_fea_lab_dict, debug1, debug2\n",
    "\n",
    "    else:\n",
    "        return X_res_test, y_res_test, test_fea_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2efaaaf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of fixed nodes: 2807\n",
      "-----------\n"
     ]
    }
   ],
   "source": [
    "# ## for test\n",
    "g_0,g_t=1977,2002\n",
    "\n",
    "fixed_test = fix_nodes(year_edge,g_0,g_t,set())\n",
    "print(\"number of fixed nodes:\", len(fixed_test))\n",
    "print(\"-----------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "24990d14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"MESH:D003329\" in fixed_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f93d420a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start of the feature network: 1977\n",
      "end of the feature network: 2000\n",
      "start of the label network: 2001\n",
      "end of the label network: 2001\n",
      "-----------\n",
      "number of fixed nodes: 2807\n",
      "-----------\n",
      "positive links in the feature network: 26000.0\n",
      "-----------\n",
      "number of links to achieve the similarity measures: 26000\n",
      "-----------\n",
      "positive links in the label network: 67751.0\n",
      "-----------\n",
      "number of links to induce the node embeddings: 93751\n",
      "-----------\n",
      "negative links in the feature network before undersampling: 1712534\n",
      "-----------\n",
      "Original dataset shape Counter({0: 1712534, 1: 67751})\n",
      "Resampled dataset shape Counter({0: 67751, 1: 67751})\n"
     ]
    }
   ],
   "source": [
    "g_f0, g_ft, g_l0, g_lt = 1977,2000,2001,2001\n",
    "X_train, y_train, train_sim, train_ne, ne_bfdup, ne_dup = data_pipeline(year_edge=year_edge, year_ln=year_ln, \n",
    "                   g_f0=g_f0, g_ft=g_ft, g_l0=g_l0, g_lt=g_lt,                                                      \n",
    "                   fixed_nodes=fixed_test,       \n",
    "#                    hubs = set(),\n",
    "                                                      sampling=True,\n",
    "                   training=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc4ebfab",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Generating Training data:\")\n",
    "# edge to train classifiers,label to train classifiers, edge to get similarity measures, edge to induce embeddings\n",
    "X_train, y_train, train_sim, train_ne = data_pipeline(year_edge, year_ln, g_f0=g_f0_tr, g_ft=g_ft_tr, \n",
    "                                 g_l0=g_l0_tr, g_lt=g_lt_tr, fixed_nodes=fixed_nodes,sampling=True,training=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9a953e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## step 2: create feature network\n",
    "# test_fea_net = create_network(year_edge,g_f0,g_ft,\n",
    "#                               fixed_nodes=fixed_test)\n",
    "\n",
    "# test_fea_dict = adj_mat_to_dict(adj_mat=remove_dup(test_fea_net),label=True)\n",
    "# test_fea_cnt = dict()\n",
    "\n",
    "# for k,v in test_fea_net.items():\n",
    "#     test_fea_cnt[k] = len(v)\n",
    "\n",
    "# # print(sum(test_fea_cnt.values()))   # duplications\n",
    "# print(\"positive links in the feature network:\", sum(test_fea_cnt.values())/2)   # without duplications\n",
    "# print(\"-----------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4d32cd9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positive links in the label network: 4309.0\n",
      "-----------\n"
     ]
    }
   ],
   "source": [
    "# ## step 3: create label network\n",
    "# # positive cases in the label network are the new links in the label network\n",
    "# # need to exclude the links with new nodes\n",
    "# test_lab_net = create_network(year_ln,g_l0,g_lt, \n",
    "#                               fixed_nodes=fixed_test, ver_idx=\"\")\n",
    "# test_lab_cnt = dict()\n",
    "\n",
    "# #     print(fixed_test)\n",
    "# #     print(test_lab_net)\n",
    "# for k,v in test_lab_net.items():\n",
    "#     test_lab_cnt[k] = len(v)\n",
    "\n",
    "# # print(sum(test_lab_cnt.values()))   # this is doubled\n",
    "# print(\"positive links in the label network:\", sum(test_lab_cnt.values())/2)\n",
    "# print(\"-----------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e53d95b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "negative links in the feature network before undersampling: 173436\n",
      "-----------\n"
     ]
    }
   ],
   "source": [
    "# ## step 4: create negative links\n",
    "# neg_label = create_negatives(year_edge, fea_pos_adj_mat=test_fea_net, \n",
    "#                              lab_pos_adj_mat=test_lab_net, fixed_nodes=fixed_test)\n",
    "# neg_label_cnt = dict()\n",
    "\n",
    "# for k,v in neg_label.items():\n",
    "#     neg_label_cnt[k] = len(v)\n",
    "\n",
    "# print(\"negative links in the feature network before undersampling:\", sum(neg_label_cnt.values()))   \n",
    "# # duplications removed, links from label network removed\n",
    "# print(\"-----------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1b19e995",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset shape Counter({0: 173436, 1: 4309})\n",
      "Resampled dataset shape Counter({0: 4309, 1: 4309})\n"
     ]
    }
   ],
   "source": [
    "# ## step 5: sample the negative links to maintain 1:1 for the positive links\n",
    "# X_res_test, y_res_test = sample_neg(test_lab_net,neg_label,sampling=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "131b3b5b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.15 ('tensorflow')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "4bd624a0593993fe43ac4046b27b898fb2ef75c21c08f81e89e64ea0f51df676"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
